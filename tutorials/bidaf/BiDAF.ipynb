{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self._module = module\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Shape -> (batch_size, time_steps, *shapes)\"\"\"\n",
    "        bs = x.shape[0]\n",
    "        ts = x.shape[1]\n",
    "        x = x.view(bs*ts, *x.shape[2:])\n",
    "        x = self._module(x)\n",
    "        x = x.view(bs, ts, *x.shape[1:])\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Pack2Pad(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(Pack2Pad, self).__init__()\n",
    "        self._module = module\n",
    "        \n",
    "    def forward(self, x, lens, ind=None):\n",
    "        # sort according to lens\n",
    "        if ind is None:\n",
    "            _, ind = torch.sort(lens, 0, descending=True)\n",
    "\n",
    "        x = rnn_utils.pack_padded_sequence(x[ind], \n",
    "                                           lens[ind], \n",
    "                                           batch_first=True)\n",
    "        # only want the first output\n",
    "        x = self._module(x)[0]\n",
    "        # reorder\n",
    "        x, _ = rnn_utils.pad_packed_sequence(x, \n",
    "                                             batch_first=True)\n",
    "        x = x[ind]\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Highway(nn.Module):\n",
    "    \"From AllenNLP\"\n",
    "    def __init__(self,\n",
    "                 in_dim: int,\n",
    "                 num_layers: int = 1):\n",
    "        super(Highway, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.layers = torch.nn.ModuleList([nn.Linear(in_dim, in_dim * 2)\n",
    "                                            for _ in range(num_layers)])\n",
    "        self.activ = nn.ReLU()\n",
    "        \n",
    "        # make bias positive to carry forward\n",
    "        for layer in self.layers:\n",
    "            layer.bias[in_dim:].data.fill_(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            f_x, gate = layer(x).chunk(2, dim=-1)\n",
    "            f_x = self.activ(f_x)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            x = gate * x + (1 - gate) * f_x\n",
    "        return x\n",
    "\n",
    "\n",
    "class BidafCharEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BidafCharEmbedding, self).__init__()\n",
    "        self.embed = nn.Embedding(262, 16, padding_idx=0)\n",
    "        self.conv = TimeDistributed(nn.Conv1d(16, 100, kernel_size=(5,), stride=(1,)))\n",
    "        self.activ = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"Takes Packed Sequence as Input\"\n",
    "        # (pack_len, seq_len)\n",
    "        x = self.embed(x)\n",
    "        # (pack_len, seq_length, in_channels) \n",
    "        x = x.transpose(-2,-1)\n",
    "        x = self.activ(self.conv(x))\n",
    "        x = x.max(-1)[0]\n",
    "        # (pack_len, out_channels) \n",
    "        return x\n",
    "\n",
    "\n",
    "class BidafEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BidafEmbedding, self).__init__()\n",
    "        self.word_embed = nn.Embedding(97914, 100)\n",
    "        self.char_embed = BidafCharEmbedding()\n",
    "        \n",
    "    def forward(self, word, char):\n",
    "        w = self.word_embed(word)\n",
    "        c = self.char_embed(char)\n",
    "        x = torch.cat([c, w], dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Matrix_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Matrix_Attention, self).__init__()\n",
    "        self.attention = nn.Linear(600, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # expand so that x,y have the same dimension\n",
    "        x = x.unsqueeze(2).expand(x.shape[0], x.shape[1], y.shape[1], x.shape[2])\n",
    "        y = y.unsqueeze(1).expand(y.shape[0], x.shape[1], y.shape[1], y.shape[2])\n",
    "        \n",
    "        # combine x and y\n",
    "        xy = torch.cat([x, y, x *y], dim=-1)\n",
    "        \n",
    "        return self.attention(xy).squeeze(-1)\n",
    "\n",
    "\n",
    "def len2mask(lens):\n",
    "    max_len = lens.max()\n",
    "    # uint8\n",
    "    mask = torch.arange(max_len).expand(len(lens), max_len) < lens.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "def masked_softmax(x, mask):\n",
    "    mask = mask.float()\n",
    "    x = torch.softmax(x * mask, dim=-1)\n",
    "    x = x * mask\n",
    "    x = x / (x.sum(-1, keepdim=True) + 1e-13)\n",
    "    return x\n",
    "\n",
    "def replace_masked_values(x, mask, value):\n",
    "    select = mask.expand(*x.shape)\n",
    "    x[select] = value\n",
    "    \n",
    "def sort_pack_seq(x):\n",
    "    \"Batch first\"\n",
    "    # get lengths of each seq\n",
    "    lengths = torch.tensor([len(i) for i in x])\n",
    "    # pad the seq\n",
    "    x = rnn_utils.pad_sequence(x, batch_first=True)\n",
    "    # sorting\n",
    "    _, ind = torch.sort(lengths, 0, descending=True)\n",
    "    \n",
    "    # packed seq\n",
    "    x = rnn_utils.pack_padded_sequence(x[ind], \n",
    "                                       lengths[ind], \n",
    "                                       batch_first=True)\n",
    "    return x, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalAttentionFlow(nn.Module):\n",
    "    \"From AllenNLP\"\n",
    "    def __init__(self):\n",
    "        super(BidirectionalAttentionFlow, self).__init__()\n",
    "\n",
    "        self.bidaf_embed = BidafEmbedding()\n",
    "        self.highway = Highway(200, num_layers=2)\n",
    "        \n",
    "        self.phrase_layer = Pack2Pad(nn.LSTM(200, 100, batch_first=True,\n",
    "                                          bidirectional=True))\n",
    "    \n",
    "        self.matrix_attention = Matrix_Attention()\n",
    "        \n",
    "        self.modeling_layer = Pack2Pad(nn.LSTM(800, 100, \n",
    "                                       num_layers=2, \n",
    "                                       batch_first=True, \n",
    "                                       dropout=0.2, \n",
    "                                       bidirectional=True))\n",
    "        \"Dense+Softmax\"\n",
    "        self.start_predictor = nn.Linear(1000, 1)\n",
    "        \n",
    "        \"LSTM+Softmax\"\n",
    "        self.end_encoder = Pack2Pad(nn.LSTM(1400, 100, \n",
    "                                    batch_first=True,\n",
    "                                    bidirectional=True))        \n",
    "        self.end_predictor = nn.Linear(1000, 1)\n",
    "\n",
    "    def forward(self, context, query):\n",
    "        \"Takes PackData as Input\"\n",
    "        \n",
    "        \"----Init Batch Calc----\"\n",
    "        # Faster if Seq lengths are variables\n",
    "        c_mask = (context.words != 0)\n",
    "        c_lens = c_mask.sum(1)\n",
    "        c_ind = torch.sort(c_lens, 0, descending=True)[1]\n",
    "        q_mask = (query.words != 0)\n",
    "        q_lens = q_mask.sum(1)\n",
    "        q_ind = torch.sort(q_lens, 0, descending=True)[1]\n",
    "\n",
    "        \n",
    "        \"----Embedding Layer----\"\n",
    "        e_c = self.highway(\n",
    "            self.bidaf_embed(context.words, context.chars))\n",
    "        e_q = self.highway(\n",
    "            self.bidaf_embed(query.words, query.chars))\n",
    "        \n",
    "        \n",
    "        \"----Phrase Layer----\"\n",
    "        e_c = self.phrase_layer(e_c, c_lens, c_ind)\n",
    "        e_q = self.phrase_layer(e_q, q_lens, q_ind)\n",
    "        \n",
    "        \n",
    "        \"----Attention Layer----\"\n",
    "        # linear attention\n",
    "        c2q_sim = self.matrix_attention(e_c , e_q)        \n",
    "        \n",
    "        # context to query attention\n",
    "        c2q_att = masked_softmax(c2q_sim, q_mask.unsqueeze(1))\n",
    "        c2q = c2q_att.bmm(e_q)\n",
    "        \n",
    "        # masked fill to value -1e7\n",
    "        q2c_sim = c2q_sim.masked_fill((1 - q_mask.unsqueeze(1)), -1e7)\n",
    "        q2c_sim = q2c_sim.max(dim=-1)[0]\n",
    "        q2c_att = masked_softmax(q2c_sim, c_mask).unsqueeze(1)\n",
    "        # Shape: c2q shape\n",
    "        q2c = q2c_att.bmm(e_c).expand(*c2q.shape)\n",
    "        \n",
    "        att_out = torch.cat([e_c,\n",
    "                             c2q,\n",
    "                             e_c * c2q,\n",
    "                             e_c * q2c],\n",
    "                             dim=-1)\n",
    "        \n",
    "        \n",
    "        \"----Modeling Layer----\"\n",
    "        model_out = self.modeling_layer(att_out, c_lens, c_ind)\n",
    "        \n",
    "        \n",
    "        \"----Output Layer----\"\n",
    "        \"----Start Layer-----\"\n",
    "        start = torch.cat([att_out, model_out], dim=-1)\n",
    "        start_logits = self.start_predictor(start).squeeze(-1)\n",
    "        start_probs = masked_softmax(start_logits, c_mask)\n",
    "        \n",
    " \n",
    "        \"----End Layer-----\"\n",
    "        start_vector = start_probs.unsqueeze(1).bmm(model_out).expand(*c2q.shape)\n",
    "        end_vector = torch.cat([att_out,\n",
    "                                model_out,\n",
    "                                start_vector,\n",
    "                                model_out * start_vector],\n",
    "                                dim=-1)\n",
    "\n",
    "        # sort according to context\n",
    "        end_out = self.end_encoder(end_vector, c_lens, c_ind)\n",
    "        end_out = torch.cat([att_out, end_out], dim=-1)\n",
    "        end_logits = self.end_predictor(end_out).squeeze(-1)\n",
    "\n",
    "        # masked fill to refine the results\n",
    "        start_logits = start_logits.masked_fill(1 - c_mask, -1e7)\n",
    "        end_logits = end_logits.masked_fill(1 - c_mask, -1e7)\n",
    "    \n",
    "        \"----Return----\"\n",
    "        return start_logits, end_logits\n",
    "\n",
    "\n",
    "    def load_weights(self, load_table, weights_dict):\n",
    "        for dst in load_table.keys():\n",
    "            self.state_dict()[dst].copy_(weights_dict[load_table[dst]])\n",
    "        \n",
    "        # set padding weight to 0\n",
    "        self.bidaf_embed.char_embed.embed.weight.data[0].fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(file_name:str)->Dict[str, str]:\n",
    "    with open(file_name, \"r\") as f:\n",
    "        load_table = f.read()\n",
    "        load_table = \"{\" + load_table + \"}\"\n",
    "        load_table =  eval(load_table)\n",
    "    return load_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = torch.load(\"weights.th\", map_location='cpu')\n",
    "load_table = read_table(\"bidaf_load.txt\")\n",
    "bidaf = BidirectionalAttentionFlow()\n",
    "bidaf.load_weights(load_table, weights_dict)\n",
    "bidaf.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp.models.archival import load_archive\n",
    "\n",
    "if not \"predictor\" in vars():\n",
    "    archive = load_archive(\"bidaf.tar.gz\")\n",
    "    predictor = Predictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackData():\n",
    "    def __init__(self, data):\n",
    "        self.words = data['tokens']\n",
    "        self.chars = data['token_characters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset import Batch\n",
    "\n",
    "instance1 = predictor._dataset_reader.text_to_instance('good', 'This is not good!')\n",
    "instance2 = predictor._dataset_reader.text_to_instance('it is bad', 'not bad')\n",
    "\n",
    "dataset = Batch([instance1, instance2])\n",
    "vocab = predictor._model.vocab\n",
    "dataset.index_instances(vocab)\n",
    "passage = dataset.as_tensor_dict()['passage']\n",
    "question = dataset.as_tensor_dict()['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = PackData(passage)\n",
    "query = PackData(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = (context.words != 0).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidaf(context, query)[1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_c, e_q = bidaf(context, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bidaf(context, query)[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-30000048., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidaf(context, query)[1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-4.1058e+00, -7.2027e+00, -5.9775e+00, -8.1707e+00, -6.1313e+00],\n",
       "         [-2.6531e+00, -4.3184e+00, -1.0000e+07, -1.0000e+07, -1.0000e+07]],\n",
       "        grad_fn=<MaskedFillBackward0>),\n",
       " tensor([[-7.6523e+00, -1.0996e+01, -8.5098e+00, -5.1956e+00, -3.5191e+00],\n",
       "         [-7.9713e+00, -4.6289e+00, -1.0000e+07, -1.0000e+07, -1.0000e+07]],\n",
       "        grad_fn=<MaskedFillBackward0>))"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidaf(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PASS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def masked_softmax(x, mask):\n",
      "    mask = mask.float()\n",
      "    x = torch.softmax(x * mask, dim=-1)\n",
      "    x = x * mask\n",
      "    x = x / (x.sum(-1, keepdim=True) + 1e-13)\n",
      "    return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(masked_softmax)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
